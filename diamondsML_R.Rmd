---
title: "Diamonds ML R Notebook"
output:
  word_document: default
<<<<<<< HEAD
  pdf_document: default
=======
  html_document: default
>>>>>>> 0bc9a6d612179184ca5cdb53bf64a8dd4455d6af
---
Diamonds ML R Notebook
Robert M. Taylor, PhD.


This notebook is to demonstrate Exploratory Data Analysis (EDA), visualizations, and machine learning in R on the diamonds dataset that is available in R. "Price" will be our target.s

I'll first import the libraries I'll use.
```{r}
library(ggplot2)
library(rpart)
library(randomForest)
library(tidyr)
library(modelr)
```

Load the dataset
```{r}
data(diamonds)
```

I'll just look at/inspect the dataset first. This is a clean data set so data cleaning will not be needed or demonstrated in this notebook.
```{r}
head(diamonds)
```

What are the dimensions of the dataset?
```{r}
dim(diamonds)
```

So there are 53,940 rows and 10 feature columns.

I'll now get 1) a summary and 2) the structure of the data...
```{r}
summary(diamonds)
```

I see that there is an (Other) variable for Clarity. I want to look at that closer.
```{r}
unique(diamonds$clarity)
```

So, everything appears good. The summary has just grouped the I1 and IF clarities in the count values in the summary table above. 

We also see that the price ranges from a minimium price of $326 to a max of $18,823

I'll go ahead and look at the structure of the dataset...

```{r}
str(diamonds)
```

I'll first do a quick and dirty look at the data...

```{r, fig.width = 10, fig.height = 10}
plot(diamonds)
```
I'll now look at caret vs price using ggplot2 

```{r}
g <- ggplot(diamonds, aes(x=carat, y=price))
g + 
  geom_point(aes(color=clarity)) +
  facet_grid(cut ~ clarity)+
  theme_bw()
```

I'll use a boxplot to look at cut vs. price. 

```{r}
diamonds$cut = as.factor(diamonds$cut)
g <- ggplot(diamonds, aes(x=cut, y=price))
g +
  geom_boxplot()+
  facet_grid(~clarity)+
  theme(axis.text.x = element_text(angle = 90, face = "bold", color = "#993333", 
                           size = 9))+
  theme(axis.text.y = element_text(face = "bold", color = "#993333"))
```
We can also look at the carat vs. color vs. price
```{r}
g <- ggplot(diamonds, aes(x=carat, y=price))
g +
geom_point(aes(color = color))
```
I'll now look at the distribution of carat and price.

```{r}
g <- ggplot(diamonds)
g +
  geom_density(aes(x=carat), fill="gray50")
```


```{r}
g <- ggplot(diamonds)
g +
  geom_density(aes(x=price), fill="gray")
```
So, thus far, we can now see that most diamonds are < 2 carats and < ~$2500 (although a second peak can be seen around $4000). 

I'll now look at the distribution of the 'x', 'y', and 'z' features and price.

```{r}
g <- ggplot(diamonds, aes(x=x, y=price))
gg <- g + geom_point(aes(color = carat, alpha=0.9))
gg + scale_color_gradient(low="blue", high="red") + theme_bw()
```


```{r}
g <- ggplot(diamonds, aes(x=y, y=price))
gg <- g + geom_point(aes(color = carat, alpha=0.9))
gg + scale_color_gradient(low="blue", high="red") + theme_bw()
```
```{r}
g <- ggplot(diamonds, aes(x=z, y=price))
gg <- g + geom_point(aes(color = carat, alpha=0.9))
gg + scale_color_gradient(low="blue", high="red") + theme_bw()
```
So, generally, higher carat = higher x, y, and z, regardless of price...which we can see a little more clear by flipping the axes.

```{r}
g <- ggplot(diamonds, aes(x=price, y=x))
gg <- g + geom_point(aes(color = carat, alpha=0.9))
gg + scale_color_gradient(low="blue", high="red") + theme_bw()
```

I'll now change categorical values to numeric in the cut, color, and clarity features.
```{r}
print("cut values")
unique(diamonds$cut)
print("clarity values")
unique(diamonds$clarity)
print("color values")
unique(diamonds$color)
```
now I'll map those categories to numerics
```{r}
diamonds = data.matrix(data.frame(unclass(diamonds)))
diamonds = data.frame(diamonds)
head(diamonds)
```

M.L.

I'll first need to split the data for the machine learning 

SPLIT THE DATA (70/30 split)


```{r}
set.seed(27)
diamond.indices <- sample(1:nrow(diamonds), 0.7*nrow(diamonds), replace=F)
diamond.train <- diamonds[diamond.indices,]
print("Train/Test 70/30 Split Dimensions")
print("train set")
dim(diamond.train)
diamond.test <- diamonds[-diamond.indices,]
print("test set")
dim(diamond.test)
```


DECISION TREE
BUILD MODEL
I'll first use a decision tree model to predict the diamond prices.

```{r}
colnames(diamonds)
```

itting decision model on training set 
```{r}
diamond.tree.model <- rpart(price ~ ., data=diamond.train)
summary(diamond.tree.model)
```

VISUALIZE THE TRAINED MODEL
```{r}
## plot the tree structure
plot(diamond.tree.model, margin=c(.25), uniform=TRUE)
title(main = "Decision Tree Model of Diamonds Data")
text(diamond.tree.model, use.n = TRUE, cex=.9)
```
make prediction using decision model
```{r}
tree.predictions <- predict(diamond.tree.model, diamond.test)
head(tree.predictions)
```


Comparison table
```{r}
diamond.comparison <- diamond.test
diamond.comparison$Predictions <- tree.predictions
head(diamond.comparison[ , c("price", "Predictions")])
```
View misclassified rows
```{r}
disagreement.index <- diamond.comparison$price != diamond.comparison$Predictions
head(diamond.comparison[disagreement.index,])
```
Extract the test data species to build the confusion matrix
```{r}
tree.confusion <- table(tree.predictions, diamond.test$price)
```


MAE (Mean absolute Error)
calculate Mean Absolute Error (MAE)
```{r}
maeTree <-mae(model = diamond.tree.model, data=diamond.test)
maeTree
```

I'll build a function to help compare MAE scores from different values for the tree depth (maxdepth)...

```{r}
# A function to get the maximum average error for a given max depth. You should pass in
# the target as the name of the target colum and the predictors as vector where each
# item in the vector is the name of the column.

get_mae <- function(maxdepth, target, predictors, training_data, testing_data){
  #turn the predictors & target into a formula to pass to rpart 
  predictors <- paste(predictors, collapse='+')
  formula <- as.formula(paste(target, "~", predictors, sep = ""))
  #build our model
  model <- rpart(formula, data=training_data, control = rpart.control(maxdepth = maxdepth))
  #get the mae
  mae <- mae(model, testing_data)
  return(mae)
}
```


```{r}
#Feed in the target and predictors
target <- "price"
predictors <- c("carat", "cut", "color", "clarity", "depth", "table", "x", "y", "z")
# get the MaE for the maxdepths between 1 and 10
for(i in 1:10){
  mae <- get_mae(maxdepth = i, target = target, predictors = predictors,
                 training_data = diamond.train, testing_data = diamond.test)
  print(glue::glue("Maxdepth: ",i, "\t MAE: ", mae))
}
```


RANDOM FOREST

```{r}
diamond.RandomForest <- randomForest(price ~ ., data=diamond.train)
maeForest <- mae(model = diamond.RandomForest, data=diamond.test)
maeForest
```

```{r}
diamond.RandomForest
```

What features does the RF think are important?
```{r}
importance(diamond.RandomForest)
```

```{r}
varImpPlot(diamond.RandomForest, sort=TRUE, col="purple")
```
MODEL EVALUATION
Predict test set outcomes, reporting class labels
```{r}
diamond.rf.predictions <- predict(diamond.RandomForest, diamond.test, type="response")
```

calculate the confusion matrix
```{r}
diamond.rf.confusion <- table(diamond.rf.predictions, diamond.test$price)
#print(diamond.rf.confusion)
```

accuracy
```{r}
diamond.rf.accuracy <- sum(diag(diamond.rf.confusion)) / sum(diamond.rf.confusion)
print(diamond.rf.accuracy)
```

LINEAR MODEL


```{r}
diamond.Linear <- lm(price ~ ., data=diamond.train)
maeLinear <- mae(model = diamond.Linear, data=diamond.test)
maeLinear
```
```{r}
summary(diamond.Linear)
```
```{r}
coef(diamond.Linear)
```


VISUALIZE THE TRAINED MODEL.
```{r}
layout(matrix(c(1,2,3,4),2,2)) # set 4 graphs/page 
plot(diamond.Linear)
```


MODEL EVALUATION
make prediction using trained model
```{r}
diamond.Linear.predictions <- predict(diamond.Linear, diamond.test)
par(mfrow=c(1,1))
plot(diamond.test$price, diamond.Linear.predictions, lwd=.4, col= 'darkred', ylim = c(0,20000), xlim = c(0, 20000))
```
```{r}
head(diamond.Linear.predictions)
```

calculate residuals
```{r}
diamond.Linear.residuals <- diamond.test$price - diamond.Linear.predictions
plot(diamond.Linear.predictions, diamond.Linear.residuals)
```
```{r}
plot(diamond.test$price, diamond.Linear.residuals)
```


calculate Root Mean Squared Error (RMSE)
```{r}
diamond.Linear.rmse <- sqrt(mean(diamond.Linear.residuals^2))
print(diamond.Linear.rmse)
```

calculate Mean Absolute Error (MAE)
```{r}
diamond.Linear.mae <- mean(abs(diamond.Linear.residuals))
print(diamond.Linear.mae)
```

R squared (coefficient of determination)
```{r}
diamond.ss.tot <- sum((diamond.test$price - mean(diamond.test$price))^2)
diamond.Linear.ss.res <- sum(diamond.Linear.residuals^2)
diamond.Linear.r2 <- 1 - diamond.Linear.ss.res / diamond.ss.tot
print(diamond.Linear.r2)
```

REPORT GENERATION

```{r}
print("Decision Tree")
maeTree
print("Random Forest")
maeForest
print("Linear Regression")
maeLinear
```

The Random Forest Model clearly did this best as far as mean absolute error.

FEATURE ENGINEERING

Let's see if some new features can improve the Random Forest Model...

```{r}
head(diamond.train)
```
I'll start by making a new feature for y per carat (z/carat). I'll label this feature as "yPerCarat"

```{r}
diamond.train$yPerCarat <- diamond.train$y / diamond.train$carat
diamond.test$yPerCarat <- diamond.test$y / diamond.test$carat
head(diamond.train, 2)
head(diamond.test, 2)
```
 I can see that the new feature was added to the datasets and calculated correctly.
 
 Now, I'll try the Random Forest Again...
 
```{r}
diamond.RandomForest2 <- randomForest(price ~ ., data=diamond.train)
maeForest2 <- mae(model = diamond.RandomForest2, data=diamond.test)
maeForest2
```

```{r}
diamond.RandomForest2
```

What features does the RF think are important?
```{r}
importance(diamond.RandomForest2)
```

```{r}
varImpPlot(diamond.RandomForest2, sort=TRUE, col="purple")
```
MODEL EVALUATION
Predict test set outcomes, reporting class labels
```{r}
diamond.rf.predictions2 <- predict(diamond.RandomForest2, diamond.test, type="response")
```

calculate the confusion matrix
```{r}
diamond.rf.confusion2 <- table(diamond.rf.predictions2, diamond.test$price)
#print(diamond.rf.confusion)
```

accuracy
```{r}
diamond.rf.accuracy2 <- sum(diag(diamond.rf.confusion2)) / sum(diamond.rf.confusion2)
print(diamond.rf.accuracy2)
```

